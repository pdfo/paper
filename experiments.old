 \section{Numerical results}
 \label{sec:numerical}

 \subsection{Comparison with methods based on finite differences}
 \label{ssec:fd}

 \subsection{Comparison on the CUTEst library}

 We first make a comparison of the \gls{pdfo}'s solvers on different problems from the CUTEst library~\cite{Gould_Orban_Toint_2015}.
 Performance profiles~\cite{Dolan_More_2002,More_Wild_2009} on unconstrained problem of dimensions at most~$10$ and~$50$ are provided respectively in Figures~\ref{fig:ppu-10} and~\ref{fig:ppu-50}.
 Broadly speaking, a performance profile plots the proportion of problem solved with respect to the number of function evaluations required to achieve convergence, in a logarithmic scale.
 The optimal value~$\obj_{\ast}$ of a given problem is considered to be the least value reached by all solvers, and an execution is considered convergent up to a tolerance~$\tau \ge 0$ whenever
 \begin{equation}
     \label{eq:cvt}
     \obj(\iter[0]) - \obj(\iter) \ge (1 - \tau) [\obj(\iter[0]) - \obj_{\ast}].
 \end{equation}
 We can observe an expected behavior; \gls{uobyqa} performs better on small problems than all others, as it is based on quadratic models obtained by fully determined interpolation, and the performances of \gls{cobyla} decrease with the dimension rise, as it uses only linear models to approximate the problem locally.

 \begin{figure}[ht]
     \begin{subfigure}{.48\textwidth}
         \centering
         \includegraphics[width=\textwidth]{perf-plain-10.pdf}
         \caption{Dimension at most~$10$.}
         \label{fig:ppu-10}
     \end{subfigure}
     \hfill
     \begin{subfigure}{.48\textwidth}
         \centering
         \includegraphics[width=\textwidth]{perf-plain-50.pdf}
         \caption{Dimension at most~$50$.}
         \label{fig:ppu-50}
     \end{subfigure}
     \caption{Performance profile on unconstrained problems with~$\tau = 10^{-4}$.}
 \end{figure}

 Consider however the following experiment.
 Given a smooth objective function~$\obj$, assume that the value received by the solvers is
 \begin{equation*}
     F_{\sigma}(x) = [1 + \sigma e(x)] \obj(x),
 \end{equation*}
 where~$e(x)$ is a random variable that follows a standard normal distribution~$\mathcal{N}(0, 1)$, and~$\sigma \ge 0$ is a given noise level.
 Figure~\ref{fig:ppun-50} presents the performance profiles on the same unconstrained problems of dimension at most~$50$ from the CUTEst library as the previous experiment by randomizing the objective functions as~$F_{\sigma}$ with~$\sigma = 10^{-2}$.
 Because of the stochastic behavior of the experiment, the convergence test~\eqref{eq:cvt} needs to be adapted.
 Each problem is solved~$10$ times by each solver, the objective value considered at each iteration is the average value for all runs, and the optimal value~$f_{\ast}$ of a given problem is decided as follows.
 It is the least value reached by the solvers for every run on the noised variation of the problem and by all solvers on the noise-free original problem.
 In doing so, one should expect a decrease in the proportion of problems solved when compared with the previous experiment.

 \begin{figure}[ht]
     \centering
     \includegraphics[width=.48\textwidth]{perf-noisy-50.pdf}
     \caption{Performance profile on noised variations of unconstrained problems of dimension at most~$50$ with a precision~$\tau = 10^{-1}$.}
     \label{fig:ppun-50}
 \end{figure}

 We observe however a peculiar behavior of \gls{cobyla} on this experiment, as it defeats all other solvers on unconstrained problems even though it is not defined for such kind of problem, and uses the simplest models.
 It seems that the linear models of \gls{cobyla} are, in some sense, less sensitive to Gaussian noise, but the authors did not derive a theory for this behavior as of today.

 \subsection{An example of hyperparameter tuning problem}

 We now consider the more practical problem of the hyperparameter tuning of a \gls{svm}.
 The model we consider is a~$C$-SVM~\cite{Chang_Lin_2011} for binary classification problems with an \gls{rbf} kernel, admitting two hyperparameters: a regularization parameter~$C > 0$ and a kernel parameter~$\gamma > 0$.
 We want to compare the performance of \gls{pdfo} with a prominent Bayesian optimization method and \gls{rs}.
 To this end, we use the Python package \texttt{hyperopt}~\cite{Bergstra_Yamins_Cox_2013} for solving the optimization problems, which provides both \gls{tpe} and \gls{rs} methods.
 Our experiments are based on binary classifications problems from the LIBSVM datasets\footnote{\url{https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/}.}.
 A description of the datasets employed is provided in Table~\ref{tab:htdata}.

 \begin{table}[ht]
     \caption{Considered LIBSVM datasets description}
     \label{tab:htdata}
     \centering
     \begin{tabular}{ccS[table-format=2]S[table-format=5]}
         \toprule
         Dataset~$\mathcal{P}$   & Attribute characteristic  & {Dimension~$d$}   & {Dataset size}\\
         \midrule
         splice                  & $[-1, 1]$, scaled         & 60                & 1000\\
         svmguide1               & $[-1, 1]$, scaled         & 4                 & 3088\\
         svmguide3               & $[-1, 1]$, scaled         & 21                & 1242\\
         ijcnn1                  & $[-1, 1]$                 & 22                & 49990\\
         \bottomrule
     \end{tabular}
 \end{table}

 The problem we consider is as follows.
 A dataset~$\mathcal{P} \subseteq [-1, 1]^d$ from Table~\ref{tab:htdata} is randomly split into a training dataset~$\mathcal{L}$, admitting approximately~$70\%$ of the data, and a testing dataset~$\mathcal{T}$, with~$\mathcal{P} = \mathcal{L} \cup \mathcal{T}$.
 We want to maximize the~$5$-fold \gls{auc} validation score of the \gls{svm} trained on~$\mathcal{L}$ with respect to the hyperparameters~$C$ and~$\gamma$.
 The \gls{auc} score, a real number in~$[0, 1]$, measures the area underneath the \gls{roc} curve, a graph representing the performance of a binary classification model.
 This curve plots the true positive classification rate with respect to the false positive classification rate at different classification thresholds.
 The~$5$-fold \gls{auc} validation score corresponds to the following.
 The set~$\mathcal{L}$ is split into~$5$ folds, and the model is trained~$5$ times, on each union of~$4$ distinct folds.
 After each training, the \gls{auc} score is calculated on the last fold, which was not involved in the training process, giving rise to~$5$ \gls{auc} scores, the average of which corresponds to the~$5$-fold \gls{auc} validation score.
 It is then clear that such an experiment lies in the \gls{dfo} context.

 The numerical results for this experiment are provided in Section~\ref{sec:htres}.
 The \gls{auc} scores and accuracies presented in the tables correspond to the ones computed on~$\mathcal{T}$ with an \gls{svm} trained on~$\mathcal{L}$, with the tuned parameters~$C$ and~$\gamma$.
 In a nutshell, it globally shows that the numerical performances of \gls{pdfo} against the two classical approaches are very similar, but the computations require much fewer \gls{auc} evaluations, and hence, much less computation time.
 This behavior is particularly visible on the dataset ``{ijcnn1}'' in Table~\ref{tab:ijcnn1}, as the size of this dataset is much larger than the others.
 Thus, we can conclude that \gls{pdfo} performed better than the package \texttt{hyperopt} on these problems, even though the final numerical results are mostly similar.
